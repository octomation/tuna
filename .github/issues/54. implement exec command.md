---
issue: 54
status: open
type: task
labels:
  - "effort: easy"
  - "impact: low"
  - "scope: code"
  - "type: feature"
assignees:
  - kamilsk
milestone: null
projects: []
relationships:
  parent: null
  blocked_by: []
  blocks: []
---

# Implement `tuna exec` command

## Description

Implement the `exec` command that executes a plan by reading `plan.toml`, parsing it into a Go structure, and making requests to an LLM using the OpenAI-compatible API.

## Usage

```bash
tuna exec <PlanID> [flags]
  --parallel, -p    Number of parallel requests (default: 1)
  --dry-run         Show what would be executed without making API calls
  --continue        Continue from last checkpoint if interrupted
```

## Expected behavior

### 1. Locate and parse the plan

The command should:
1. Search for `plan.toml` by PlanID using glob pattern `*/Output/<PlanID>/plan.toml`
2. Parse TOML into a Go struct
3. Validate that `plan_id` in the file matches the provided PlanID

**Example `plan.toml`:**
```toml
plan_id = "d9c35d53-288b-4bd4-ae44-572336ef7713"
assistant_id = "OKR Assistant"

[assistant]
system_prompt = """
Your system prompt here...
"""

[assistant.llm]
models = ["qwen3-32b"]
max_tokens = 4096
temperature = 0.7

[[query]]
id = "048bb08b-470f-4014-aee8-b1235d4b7a5f.md"
```

**Plan structure (`internal/plan/plan.go`):**
```go
type Plan struct {
    PlanID      string    `toml:"plan_id"`
    AssistantID string    `toml:"assistant_id"`
    Assistant   Assistant `toml:"assistant"`
    Queries     []Query   `toml:"query"`
}

type Assistant struct {
    SystemPrompt string `toml:"system_prompt"`
    LLM          LLM    `toml:"llm"`
}

type LLM struct {
    Models      []string `toml:"models"`
    MaxTokens   int      `toml:"max_tokens"`
    Temperature float64  `toml:"temperature"`
}

type Query struct {
    ID string `toml:"id"`
}
```

### 2. Execute LLM requests (MVP)

For MVP implementation:
- Read the **first** query file from `<AssistantID>/Input/` directory
- Use the **first** model from `assistant.llm.models` array
- Send request with system prompt from plan and query file content as user message
- Print response to stdout

**Note:** Full implementation (multiple queries, multiple models, saving responses to files) will be added in a future iteration.

**Use existing library [`github.com/sashabaranov/go-openai`](https://github.com/sashabaranov/go-openai):**
```go
import openai "github.com/sashabaranov/go-openai"

// Create client with custom base URL for OpenAI-compatible APIs
config := openai.DefaultConfig(apiKey)
config.BaseURL = baseURL // from LLM_BASE_URL env var
client := openai.NewClientWithConfig(config)

// Make chat completion request
resp, err := client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
    Model: model,
    Messages: []openai.ChatCompletionMessage{
        {Role: openai.ChatMessageRoleSystem, Content: systemPrompt},
        {Role: openai.ChatMessageRoleUser, Content: userQuery},
    },
    Temperature: temperature,
    MaxTokens:   maxTokens,
})
```

### 3. Configuration

Read API configuration from environment variables (vendor-neutral naming):
- `LLM_API_TOKEN` - API token for authentication (required)
- `LLM_BASE_URL` - Base URL for API (required, no default)

## Implementation details

### New packages

1. `internal/plan` - Plan parsing and validation

### Dependencies

Add to `go.mod`:
- `github.com/sashabaranov/go-openai` - OpenAI-compatible client
- `github.com/BurntSushi/toml` - TOML parser

### Dry-run mode

When `--dry-run` is specified:
- Parse and validate the plan
- Print what would be executed:
  - Plan ID and Assistant ID
  - Models list from `assistant.llm.models`
  - Query IDs from `query` array
  - LLM parameters (temperature, max_tokens)
- Do NOT make actual API calls

### Flags implementation (MVP)

| Flag             | MVP Status                                               |
|------------------|----------------------------------------------------------|
| `--dry-run`      | Implemented                                              |
| `--parallel, -p` | Stub: prints "not implemented" warning, uses default (1) |
| `--continue`     | Stub: prints "not implemented" warning                   |

## Edge Cases

- Plan file not found by PlanID → error with suggestion to run `tuna plan` first
- Invalid TOML syntax → error with parse details
- `plan_id` in file doesn't match PlanID argument → error
- Missing `LLM_API_TOKEN` env var → error with setup instructions
- Missing `LLM_BASE_URL` env var → error with setup instructions
- Empty `models` array in plan → error
- Query file referenced in plan not found → error with file path
- API request fails → error with response details (status code, message)

## Acceptance criteria

### Core functionality
- [ ] Plan is located by PlanID using glob pattern
- [ ] Plan is parsed from `plan.toml` into a typed Go struct
- [ ] `plan_id` in file is validated against PlanID argument
- [ ] Query file content is read from `<AssistantID>/Input/`

### LLM integration
- [ ] LLM requests use `github.com/sashabaranov/go-openai` library
- [ ] Command reads `LLM_API_TOKEN` and `LLM_BASE_URL` from environment
- [ ] Command makes a successful request using first model and first query
- [ ] Response is printed to stdout

### Flags
- [ ] `--dry-run` shows execution plan without API calls
- [ ] `--parallel` prints "not implemented" warning
- [ ] `--continue` prints "not implemented" warning

### Error handling
- [ ] Clear error for missing plan file
- [ ] Clear error for missing environment variables
- [ ] Clear error for invalid TOML or mismatched plan_id

---

# Plan: Implement `tuna exec` command

## Overview

This plan describes the implementation steps for the `exec` command that executes a plan by making requests to an LLM using the OpenAI-compatible API. The stub command already exists in `internal/command/exec.go` and needs to be extended with actual functionality.

## Prerequisites

- Existing stub command in `internal/command/exec.go`
- Package `internal/plan/` from issue #53 (Plan struct)
- External: `github.com/sashabaranov/go-openai` for LLM API
- External: `github.com/pelletier/go-toml/v2` (already in go.mod) for TOML parsing

## Steps

### 1. Add OpenAI client dependency

```bash
go get github.com/sashabaranov/go-openai
```

### 2. Extend plan package with loader

**File:** `internal/plan/loader.go`

```go
package plan

import (
    "fmt"
    "os"
    "path/filepath"

    "github.com/pelletier/go-toml/v2"
)

// Load finds and parses a plan by its ID.
// Searches for plan.toml using glob pattern: */Output/<planID>/plan.toml
func Load(baseDir, planID string) (*Plan, string, error) {
    pattern := filepath.Join(baseDir, "*", "Output", planID, "plan.toml")

    matches, err := filepath.Glob(pattern)
    if err != nil {
        return nil, "", fmt.Errorf("failed to search for plan: %w", err)
    }

    if len(matches) == 0 {
        return nil, "", fmt.Errorf("plan not found: %s\nRun 'tuna plan <AssistantID>' to create a plan first", planID)
    }

    if len(matches) > 1 {
        return nil, "", fmt.Errorf("multiple plans found with ID %s: %v", planID, matches)
    }

    planPath := matches[0]

    data, err := os.ReadFile(planPath)
    if err != nil {
        return nil, "", fmt.Errorf("failed to read plan file: %w", err)
    }

    var plan Plan
    if err := toml.Unmarshal(data, &plan); err != nil {
        return nil, "", fmt.Errorf("failed to parse plan.toml: %w", err)
    }

    if plan.PlanID != planID {
        return nil, "", fmt.Errorf("plan_id mismatch: expected %s, got %s", planID, plan.PlanID)
    }

    return &plan, planPath, nil
}

// AssistantDir returns the assistant directory path from plan.toml path.
func AssistantDir(planPath string) string {
    // planPath: <base>/<AssistantID>/Output/<planID>/plan.toml
    // Go up 3 levels to get AssistantID directory
    return filepath.Dir(filepath.Dir(filepath.Dir(planPath)))
}
```

### 3. Create LLM client package

**File:** `internal/llm/client.go`

```go
package llm

import (
    "context"
    "fmt"
    "os"

    openai "github.com/sashabaranov/go-openai"
)

const (
    EnvAPIToken = "LLM_API_TOKEN"
    EnvBaseURL  = "LLM_BASE_URL"
)

// Config holds LLM client configuration.
type Config struct {
    APIToken string
    BaseURL  string
}

// ConfigFromEnv reads LLM configuration from environment variables.
func ConfigFromEnv() (*Config, error) {
    token := os.Getenv(EnvAPIToken)
    if token == "" {
        return nil, fmt.Errorf("missing %s environment variable\n\nSet it with:\n  export %s=your-api-token", EnvAPIToken, EnvAPIToken)
    }

    baseURL := os.Getenv(EnvBaseURL)
    if baseURL == "" {
        return nil, fmt.Errorf("missing %s environment variable\n\nSet it with:\n  export %s=https://api.example.com/v1", EnvBaseURL, EnvBaseURL)
    }

    return &Config{
        APIToken: token,
        BaseURL:  baseURL,
    }, nil
}

// Client wraps the OpenAI client for LLM interactions.
type Client struct {
    client *openai.Client
}

// NewClient creates a new LLM client with the given configuration.
func NewClient(cfg *Config) *Client {
    config := openai.DefaultConfig(cfg.APIToken)
    config.BaseURL = cfg.BaseURL

    return &Client{
        client: openai.NewClientWithConfig(config),
    }
}

// ChatRequest holds parameters for a chat completion request.
type ChatRequest struct {
    Model        string
    SystemPrompt string
    UserMessage  string
    Temperature  float64
    MaxTokens    int
}

// ChatResponse holds the response from a chat completion.
type ChatResponse struct {
    Content      string
    Model        string
    PromptTokens int
    OutputTokens int
}

// Chat sends a chat completion request and returns the response.
func (c *Client) Chat(ctx context.Context, req ChatRequest) (*ChatResponse, error) {
    resp, err := c.client.CreateChatCompletion(ctx, openai.ChatCompletionRequest{
        Model: req.Model,
        Messages: []openai.ChatCompletionMessage{
            {Role: openai.ChatMessageRoleSystem, Content: req.SystemPrompt},
            {Role: openai.ChatMessageRoleUser, Content: req.UserMessage},
        },
        Temperature: float32(req.Temperature),
        MaxTokens:   req.MaxTokens,
    })
    if err != nil {
        return nil, fmt.Errorf("chat completion failed: %w", err)
    }

    if len(resp.Choices) == 0 {
        return nil, fmt.Errorf("no response choices returned")
    }

    return &ChatResponse{
        Content:      resp.Choices[0].Message.Content,
        Model:        resp.Model,
        PromptTokens: resp.Usage.PromptTokens,
        OutputTokens: resp.Usage.CompletionTokens,
    }, nil
}
```

### 4. Create executor package

**File:** `internal/exec/executor.go`

```go
package exec

import (
    "context"
    "fmt"
    "os"
    "path/filepath"

    "go.octolab.org/template/tool/internal/assistant"
    "go.octolab.org/template/tool/internal/llm"
    "go.octolab.org/template/tool/internal/plan"
)

// Options holds execution options.
type Options struct {
    DryRun   bool
    Parallel int
    Continue bool
}

// Result holds execution result.
type Result struct {
    Response     string
    Model        string
    QueryID      string
    PromptTokens int
    OutputTokens int
}

// Executor handles plan execution.
type Executor struct {
    plan        *plan.Plan
    assistantDir string
    llmClient   *llm.Client
    options     Options
}

// New creates a new executor for the given plan.
func New(p *plan.Plan, assistantDir string, llmClient *llm.Client, opts Options) *Executor {
    return &Executor{
        plan:         p,
        assistantDir: assistantDir,
        llmClient:    llmClient,
        options:      opts,
    }
}

// DryRun prints what would be executed without making API calls.
func (e *Executor) DryRun() string {
    var output string

    output += fmt.Sprintf("Plan ID:      %s\n", e.plan.PlanID)
    output += fmt.Sprintf("Assistant ID: %s\n", e.plan.AssistantID)
    output += "\n"

    output += "Models:\n"
    for i, model := range e.plan.Assistant.LLM.Models {
        marker := "  "
        if i == 0 {
            marker = "* " // MVP: first model will be used
        }
        output += fmt.Sprintf("  %s%s\n", marker, model)
    }
    output += "\n"

    output += "Queries:\n"
    for i, query := range e.plan.Queries {
        marker := "  "
        if i == 0 {
            marker = "* " // MVP: first query will be used
        }
        output += fmt.Sprintf("  %s%s\n", marker, query.ID)
    }
    output += "\n"

    output += "LLM Parameters:\n"
    output += fmt.Sprintf("  Temperature: %.1f\n", e.plan.Assistant.LLM.Temperature)
    output += fmt.Sprintf("  Max tokens:  %d\n", e.plan.Assistant.LLM.MaxTokens)
    output += "\n"

    output += "(MVP: only first model and first query will be executed)\n"

    return output
}

// Execute runs the plan (MVP: first query with first model).
func (e *Executor) Execute(ctx context.Context) (*Result, error) {
    // Validate plan has required data
    if len(e.plan.Assistant.LLM.Models) == 0 {
        return nil, fmt.Errorf("no models specified in plan")
    }
    if len(e.plan.Queries) == 0 {
        return nil, fmt.Errorf("no queries specified in plan")
    }

    // MVP: use first model and first query
    model := e.plan.Assistant.LLM.Models[0]
    queryID := e.plan.Queries[0].ID

    // Read query file
    queryPath := filepath.Join(e.assistantDir, "Input", queryID)
    queryContent, err := os.ReadFile(queryPath)
    if err != nil {
        return nil, fmt.Errorf("failed to read query file %s: %w", queryPath, err)
    }

    // Make LLM request
    resp, err := e.llmClient.Chat(ctx, llm.ChatRequest{
        Model:        model,
        SystemPrompt: e.plan.Assistant.SystemPrompt,
        UserMessage:  string(queryContent),
        Temperature:  e.plan.Assistant.LLM.Temperature,
        MaxTokens:    e.plan.Assistant.LLM.MaxTokens,
    })
    if err != nil {
        return nil, err
    }

    return &Result{
        Response:     resp.Content,
        Model:        resp.Model,
        QueryID:      queryID,
        PromptTokens: resp.PromptTokens,
        OutputTokens: resp.OutputTokens,
    }, nil
}
```

### 5. Update exec command

**File:** `internal/command/exec.go`

```go
package command

import (
    "context"
    "fmt"
    "os"

    "github.com/spf13/cobra"

    "go.octolab.org/template/tool/internal/exec"
    "go.octolab.org/template/tool/internal/llm"
    "go.octolab.org/template/tool/internal/plan"
)

// Exec returns a cobra.Command to execute a plan.
func Exec() *cobra.Command {
    var (
        parallel   int
        dryRun     bool
        continueOp bool
    )

    command := cobra.Command{
        Use:   "exec <PlanID>",
        Short: "Execute a plan",
        Long: `Execute runs the specified plan, sending queries to the configured models.

Environment variables required:
  LLM_API_TOKEN  API token for authentication
  LLM_BASE_URL   Base URL for OpenAI-compatible API

MVP: Executes only the first query with the first model.`,

        Args: cobra.ExactArgs(1),
        RunE: func(cmd *cobra.Command, args []string) error {
            planID := args[0]

            // Warn about unimplemented flags
            if parallel > 1 {
                cmd.PrintErrln("Warning: --parallel is not yet implemented, using default (1)")
            }
            if continueOp {
                cmd.PrintErrln("Warning: --continue is not yet implemented")
            }

            // Get working directory
            cwd, err := os.Getwd()
            if err != nil {
                return fmt.Errorf("failed to get working directory: %w", err)
            }

            // Load plan
            p, planPath, err := plan.Load(cwd, planID)
            if err != nil {
                return err
            }

            assistantDir := plan.AssistantDir(planPath)

            // Dry run mode
            if dryRun {
                executor := exec.New(p, assistantDir, nil, exec.Options{DryRun: true})
                cmd.Print(executor.DryRun())
                return nil
            }

            // Load LLM config from environment
            llmCfg, err := llm.ConfigFromEnv()
            if err != nil {
                return err
            }

            // Create LLM client
            llmClient := llm.NewClient(llmCfg)

            // Execute
            executor := exec.New(p, assistantDir, llmClient, exec.Options{
                DryRun:   dryRun,
                Parallel: parallel,
                Continue: continueOp,
            })

            ctx := context.Background()
            result, err := executor.Execute(ctx)
            if err != nil {
                return err
            }

            // Print result
            cmd.Printf("Query: %s\n", result.QueryID)
            cmd.Printf("Model: %s\n", result.Model)
            cmd.Printf("Tokens: %d prompt + %d output = %d total\n",
                result.PromptTokens, result.OutputTokens,
                result.PromptTokens+result.OutputTokens)
            cmd.Println()
            cmd.Println("--- Response ---")
            cmd.Println(result.Response)

            return nil
        },
    }

    command.Flags().IntVarP(&parallel, "parallel", "p", 1, "Number of parallel requests")
    command.Flags().BoolVar(&dryRun, "dry-run", false, "Show what would be executed without making API calls")
    command.Flags().BoolVar(&continueOp, "continue", false, "Continue from last checkpoint if interrupted")

    return &command
}
```

### 6. Add unit tests for plan loader

**File:** `internal/plan/loader_test.go`

```go
package plan

import (
    "os"
    "path/filepath"
    "testing"
)

func TestLoad(t *testing.T) {
    t.Run("loads valid plan", func(t *testing.T) {
        tmpDir := t.TempDir()

        // Create plan structure
        planID := "test-plan-id"
        planDir := filepath.Join(tmpDir, "test-assistant", "Output", planID)
        os.MkdirAll(planDir, 0755)

        planContent := `
plan_id = "test-plan-id"
assistant_id = "test-assistant"

[assistant]
system_prompt = "Test prompt"

[assistant.llm]
models = ["gpt-4"]
max_tokens = 1000
temperature = 0.5

[[query]]
id = "query.md"
`
        os.WriteFile(filepath.Join(planDir, "plan.toml"), []byte(planContent), 0644)

        plan, planPath, err := Load(tmpDir, planID)
        if err != nil {
            t.Fatalf("Load() error = %v", err)
        }

        if plan.PlanID != planID {
            t.Errorf("PlanID = %q, want %q", plan.PlanID, planID)
        }
        if plan.AssistantID != "test-assistant" {
            t.Errorf("AssistantID = %q, want %q", plan.AssistantID, "test-assistant")
        }
        if planPath == "" {
            t.Error("planPath should not be empty")
        }
    })

    t.Run("returns error for missing plan", func(t *testing.T) {
        tmpDir := t.TempDir()

        _, _, err := Load(tmpDir, "nonexistent")
        if err == nil {
            t.Error("Expected error for missing plan")
        }
    })

    t.Run("returns error for plan_id mismatch", func(t *testing.T) {
        tmpDir := t.TempDir()

        planDir := filepath.Join(tmpDir, "assistant", "Output", "wrong-id")
        os.MkdirAll(planDir, 0755)

        planContent := `plan_id = "different-id"`
        os.WriteFile(filepath.Join(planDir, "plan.toml"), []byte(planContent), 0644)

        _, _, err := Load(tmpDir, "wrong-id")
        if err == nil {
            t.Error("Expected error for plan_id mismatch")
        }
    })
}

func TestAssistantDir(t *testing.T) {
    tests := []struct {
        planPath string
        expected string
    }{
        {"/base/Assistant/Output/plan-id/plan.toml", "/base/Assistant"},
        {"./My Assistant/Output/123/plan.toml", "My Assistant"},
    }

    for _, tt := range tests {
        result := AssistantDir(tt.planPath)
        if result != tt.expected {
            t.Errorf("AssistantDir(%q) = %q, want %q", tt.planPath, result, tt.expected)
        }
    }
}
```

### 7. Add unit tests for LLM client

**File:** `internal/llm/client_test.go`

```go
package llm

import (
    "os"
    "testing"
)

func TestConfigFromEnv(t *testing.T) {
    t.Run("returns config when vars set", func(t *testing.T) {
        os.Setenv(EnvAPIToken, "test-token")
        os.Setenv(EnvBaseURL, "https://api.example.com")
        defer os.Unsetenv(EnvAPIToken)
        defer os.Unsetenv(EnvBaseURL)

        cfg, err := ConfigFromEnv()
        if err != nil {
            t.Fatalf("ConfigFromEnv() error = %v", err)
        }

        if cfg.APIToken != "test-token" {
            t.Errorf("APIToken = %q, want %q", cfg.APIToken, "test-token")
        }
        if cfg.BaseURL != "https://api.example.com" {
            t.Errorf("BaseURL = %q, want %q", cfg.BaseURL, "https://api.example.com")
        }
    })

    t.Run("returns error for missing token", func(t *testing.T) {
        os.Unsetenv(EnvAPIToken)
        os.Setenv(EnvBaseURL, "https://api.example.com")
        defer os.Unsetenv(EnvBaseURL)

        _, err := ConfigFromEnv()
        if err == nil {
            t.Error("Expected error for missing API token")
        }
    })

    t.Run("returns error for missing base URL", func(t *testing.T) {
        os.Setenv(EnvAPIToken, "test-token")
        os.Unsetenv(EnvBaseURL)
        defer os.Unsetenv(EnvAPIToken)

        _, err := ConfigFromEnv()
        if err == nil {
            t.Error("Expected error for missing base URL")
        }
    })
}
```

### 8. Add unit tests for executor

**File:** `internal/exec/executor_test.go`

```go
package exec

import (
    "strings"
    "testing"

    "go.octolab.org/template/tool/internal/plan"
)

func TestDryRun(t *testing.T) {
    p := &plan.Plan{
        PlanID:      "test-plan",
        AssistantID: "test-assistant",
        Assistant: plan.Assistant{
            SystemPrompt: "Test prompt",
            LLM: plan.LLM{
                Models:      []string{"gpt-4", "claude-3"},
                MaxTokens:   4096,
                Temperature: 0.7,
            },
        },
        Queries: []plan.Query{
            {ID: "query1.md"},
            {ID: "query2.md"},
        },
    }

    executor := New(p, "/tmp/assistant", nil, Options{DryRun: true})
    output := executor.DryRun()

    expectedStrings := []string{
        "Plan ID:      test-plan",
        "Assistant ID: test-assistant",
        "gpt-4",
        "claude-3",
        "query1.md",
        "query2.md",
        "Temperature: 0.7",
        "Max tokens:  4096",
    }

    for _, expected := range expectedStrings {
        if !strings.Contains(output, expected) {
            t.Errorf("DryRun output missing %q", expected)
        }
    }
}

func TestExecuteValidation(t *testing.T) {
    t.Run("fails for empty models", func(t *testing.T) {
        p := &plan.Plan{
            Assistant: plan.Assistant{
                LLM: plan.LLM{Models: []string{}},
            },
            Queries: []plan.Query{{ID: "q.md"}},
        }

        executor := New(p, "/tmp", nil, Options{})
        _, err := executor.Execute(nil)
        if err == nil {
            t.Error("Expected error for empty models")
        }
    })

    t.Run("fails for empty queries", func(t *testing.T) {
        p := &plan.Plan{
            Assistant: plan.Assistant{
                LLM: plan.LLM{Models: []string{"gpt-4"}},
            },
            Queries: []plan.Query{},
        }

        executor := New(p, "/tmp", nil, Options{})
        _, err := executor.Execute(nil)
        if err == nil {
            t.Error("Expected error for empty queries")
        }
    })
}
```

### 9. Verify implementation

- [ ] Run `go get github.com/sashabaranov/go-openai` to add dependency
- [ ] Run `go build` to ensure no compilation errors
- [ ] Run `go test ./internal/plan/...` to verify plan tests
- [ ] Run `go test ./internal/llm/...` to verify LLM client tests
- [ ] Run `go test ./internal/exec/...` to verify executor tests
- [ ] Create test assistant and plan:
  ```bash
  tuna init test-assistant
  tuna plan test-assistant
  ```
- [ ] Test dry-run mode: `tuna exec <plan-id> --dry-run`
- [ ] Test with real API (requires LLM_API_TOKEN and LLM_BASE_URL):
  ```bash
  export LLM_API_TOKEN=your-token
  export LLM_BASE_URL=https://api.openai.com/v1
  tuna exec <plan-id>
  ```
- [ ] Verify warning messages for --parallel and --continue flags

## File Changes Summary

| File                             | Action |
|----------------------------------|--------|
| `go.mod`                         | Modify |
| `go.sum`                         | Modify |
| `internal/plan/loader.go`        | Create |
| `internal/plan/loader_test.go`   | Create |
| `internal/llm/client.go`         | Create |
| `internal/llm/client_test.go`    | Create |
| `internal/exec/executor.go`      | Create |
| `internal/exec/executor_test.go` | Create |
| `internal/command/exec.go`       | Modify |

## Notes

- MVP scope: only first model and first query are executed
- `--parallel` and `--continue` flags print warnings but don't affect execution
- LLM client uses `sashabaranov/go-openai` which supports any OpenAI-compatible API
- Environment variables use vendor-neutral naming (`LLM_*` not `OPENAI_*`)
- Plan loader uses glob pattern to find plan by ID across all assistants
- Error messages include actionable suggestions (e.g., "run tuna plan first")
- Dry-run mode marks MVP items with `*` to show what will be executed
- Response output includes token usage statistics for cost tracking

---

# Phase 2: Full execution with file output

## Overview

This phase extends the MVP to execute **all queries** with **all models** and save results to files according to the project specification.

## Output structure

According to `CLAUDE.md`, responses must be saved in:

```
{AssistantID}/Output/{plan_id}/{model_hash}/
├── {query_id}_response.md
├── ...
└── (one file per query)
```

Where:
- `{AssistantID}` - assistant folder name (e.g., `OKR Assistant`)
- `{plan_id}` - plan UUID (e.g., `d9c35d53-288b-4bd4-ae44-572336ef7713`)
- `{model_hash}` - short hash derived from model name for directory naming
- `{query_id}_response.md` - response file with query ID as prefix

## Steps

### 1. Add model hash utility

**File:** `internal/exec/hash.go`

```go
package exec

import (
    "crypto/sha256"
    "encoding/hex"
)

// ModelHash generates a short hash from model name for directory naming.
// Returns first 8 characters of SHA-256 hash.
func ModelHash(model string) string {
    hash := sha256.Sum256([]byte(model))
    return hex.EncodeToString(hash[:])[:8]
}
```

### 2. Add response writer

**File:** `internal/exec/writer.go`

```go
package exec

import (
    "fmt"
    "os"
    "path/filepath"
    "strings"
)

// ResponseWriter handles saving LLM responses to files.
type ResponseWriter struct {
    baseDir string // {AssistantID}/Output/{plan_id}
}

// NewResponseWriter creates a writer for the given plan output directory.
func NewResponseWriter(assistantDir, planID string) *ResponseWriter {
    return &ResponseWriter{
        baseDir: filepath.Join(assistantDir, "Output", planID),
    }
}

// Write saves a response to the appropriate file.
// Path: {baseDir}/{model_hash}/{query_id}_response.md
func (w *ResponseWriter) Write(model, queryID, content string) (string, error) {
    modelDir := filepath.Join(w.baseDir, ModelHash(model))

    // Create model directory if not exists
    if err := os.MkdirAll(modelDir, 0755); err != nil {
        return "", fmt.Errorf("failed to create output directory: %w", err)
    }

    // Build response filename: query_001.md -> query_001_response.md
    baseName := strings.TrimSuffix(queryID, filepath.Ext(queryID))
    responseFile := baseName + "_response.md"
    responsePath := filepath.Join(modelDir, responseFile)

    // Write response content
    if err := os.WriteFile(responsePath, []byte(content), 0644); err != nil {
        return "", fmt.Errorf("failed to write response file: %w", err)
    }

    return responsePath, nil
}
```

### 3. Update Result struct to support multiple results

**File:** `internal/exec/executor.go` (modifications)

```go
// Result holds execution result for a single query-model pair.
type Result struct {
    Response     string
    Model        string
    QueryID      string
    OutputPath   string // Path where response was saved
    PromptTokens int
    OutputTokens int
}

// ExecutionSummary holds results for the entire plan execution.
type ExecutionSummary struct {
    Results      []Result
    TotalQueries int
    TotalModels  int
    TotalTokens  struct {
        Prompt int
        Output int
    }
    Errors []error
}
```

### 4. Update Execute method to process all queries and models

**File:** `internal/exec/executor.go` (updated Execute method)

```go
// Execute runs the plan for all queries and all models.
func (e *Executor) Execute(ctx context.Context) (*ExecutionSummary, error) {
    // Validate plan
    if len(e.plan.Assistant.LLM.Models) == 0 {
        return nil, fmt.Errorf("no models specified in plan")
    }
    if len(e.plan.Queries) == 0 {
        return nil, fmt.Errorf("no queries specified in plan")
    }

    writer := NewResponseWriter(e.assistantDir, e.plan.PlanID)
    summary := &ExecutionSummary{
        TotalQueries: len(e.plan.Queries),
        TotalModels:  len(e.plan.Assistant.LLM.Models),
    }

    // Iterate over all models
    for _, model := range e.plan.Assistant.LLM.Models {
        // Iterate over all queries
        for _, query := range e.plan.Queries {
            result, err := e.executeOne(ctx, model, query.ID, writer)
            if err != nil {
                summary.Errors = append(summary.Errors, fmt.Errorf(
                    "model=%s query=%s: %w", model, query.ID, err,
                ))
                continue
            }

            summary.Results = append(summary.Results, *result)
            summary.TotalTokens.Prompt += result.PromptTokens
            summary.TotalTokens.Output += result.OutputTokens
        }
    }

    return summary, nil
}

// executeOne runs a single query with a single model.
func (e *Executor) executeOne(ctx context.Context, model, queryID string, writer *ResponseWriter) (*Result, error) {
    // Read query file
    queryPath := filepath.Join(e.assistantDir, "Input", queryID)
    queryContent, err := os.ReadFile(queryPath)
    if err != nil {
        return nil, fmt.Errorf("failed to read query file %s: %w", queryPath, err)
    }

    // Make LLM request
    resp, err := e.llmClient.Chat(ctx, llm.ChatRequest{
        Model:        model,
        SystemPrompt: e.plan.Assistant.SystemPrompt,
        UserMessage:  string(queryContent),
        Temperature:  e.plan.Assistant.LLM.Temperature,
        MaxTokens:    e.plan.Assistant.LLM.MaxTokens,
    })
    if err != nil {
        return nil, err
    }

    // Save response to file
    outputPath, err := writer.Write(model, queryID, resp.Content)
    if err != nil {
        return nil, err
    }

    return &Result{
        Response:     resp.Content,
        Model:        resp.Model,
        QueryID:      queryID,
        OutputPath:   outputPath,
        PromptTokens: resp.PromptTokens,
        OutputTokens: resp.OutputTokens,
    }, nil
}
```

### 5. Update command output format

**File:** `internal/command/exec.go` (updated result printing)

```go
// Print summary
cmd.Printf("Execution complete\n\n")
cmd.Printf("Plan:      %s\n", planID)
cmd.Printf("Queries:   %d\n", summary.TotalQueries)
cmd.Printf("Models:    %d\n", summary.TotalModels)
cmd.Printf("Tokens:    %d prompt + %d output = %d total\n\n",
    summary.TotalTokens.Prompt,
    summary.TotalTokens.Output,
    summary.TotalTokens.Prompt+summary.TotalTokens.Output)

cmd.Println("Results:")
for _, result := range summary.Results {
    cmd.Printf("  ✓ %s → %s\n", result.QueryID, result.OutputPath)
}

if len(summary.Errors) > 0 {
    cmd.Println("\nErrors:")
    for _, err := range summary.Errors {
        cmd.Printf("  ✗ %s\n", err)
    }
}
```

### 6. Update DryRun to show all planned executions

```go
func (e *Executor) DryRun() string {
    var output string

    output += fmt.Sprintf("Plan ID:      %s\n", e.plan.PlanID)
    output += fmt.Sprintf("Assistant ID: %s\n\n", e.plan.AssistantID)

    output += "Execution matrix:\n"
    for _, model := range e.plan.Assistant.LLM.Models {
        hash := ModelHash(model)
        output += fmt.Sprintf("\n  Model: %s (hash: %s)\n", model, hash)
        for _, query := range e.plan.Queries {
            baseName := strings.TrimSuffix(query.ID, filepath.Ext(query.ID))
            outputPath := fmt.Sprintf("Output/%s/%s/%s_response.md",
                e.plan.PlanID, hash, baseName)
            output += fmt.Sprintf("    %s → %s\n", query.ID, outputPath)
        }
    }

    output += "\nLLM Parameters:\n"
    output += fmt.Sprintf("  Temperature: %.1f\n", e.plan.Assistant.LLM.Temperature)
    output += fmt.Sprintf("  Max tokens:  %d\n\n", e.plan.Assistant.LLM.MaxTokens)

    total := len(e.plan.Assistant.LLM.Models) * len(e.plan.Queries)
    output += fmt.Sprintf("Total requests: %d (%d models × %d queries)\n",
        total, len(e.plan.Assistant.LLM.Models), len(e.plan.Queries))

    return output
}
```

### 7. Add tests for new functionality

**File:** `internal/exec/hash_test.go`

```go
package exec

import "testing"

func TestModelHash(t *testing.T) {
    tests := []struct {
        model    string
        expected string
    }{
        {"gpt-4", "4c8e86ae"},           // verify actual hash
        {"claude-3-opus", "xxxxxxxx"},   // verify actual hash
    }

    for _, tt := range tests {
        hash := ModelHash(tt.model)
        if len(hash) != 8 {
            t.Errorf("ModelHash(%q) = %q, want 8 chars", tt.model, hash)
        }
    }

    // Same input produces same hash
    h1 := ModelHash("gpt-4")
    h2 := ModelHash("gpt-4")
    if h1 != h2 {
        t.Errorf("ModelHash not deterministic: %q != %q", h1, h2)
    }

    // Different inputs produce different hashes
    h3 := ModelHash("gpt-3.5")
    if h1 == h3 {
        t.Errorf("ModelHash collision: %q and %q both produce %q", "gpt-4", "gpt-3.5", h1)
    }
}
```

**File:** `internal/exec/writer_test.go`

```go
package exec

import (
    "os"
    "path/filepath"
    "testing"
)

func TestResponseWriter(t *testing.T) {
    tmpDir := t.TempDir()

    writer := NewResponseWriter(tmpDir, "test-plan-id")

    // Write response
    path, err := writer.Write("gpt-4", "query_001.md", "Test response content")
    if err != nil {
        t.Fatalf("Write() error = %v", err)
    }

    // Verify file was created
    expectedPath := filepath.Join(tmpDir, "Output", "test-plan-id",
        ModelHash("gpt-4"), "query_001_response.md")
    if path != expectedPath {
        t.Errorf("Write() path = %q, want %q", path, expectedPath)
    }

    // Verify content
    content, err := os.ReadFile(path)
    if err != nil {
        t.Fatalf("ReadFile() error = %v", err)
    }
    if string(content) != "Test response content" {
        t.Errorf("File content = %q, want %q", string(content), "Test response content")
    }
}
```

### 8. Verification checklist

- [ ] Run `go build` to verify compilation
- [ ] Run `go test ./internal/exec/...` for new tests
- [ ] Test with dry-run: `tuna exec <plan-id> --dry-run`
  - Verify execution matrix shows all models × queries
  - Verify output paths are shown correctly
- [ ] Test full execution:
  ```bash
  export LLM_API_TOKEN=your-token
  export LLM_BASE_URL=https://api.example.com/v1
  tuna exec <plan-id>
  ```
- [ ] Verify output files are created:
  ```
  {AssistantID}/Output/{plan_id}/{model_hash}/query_*_response.md
  ```
- [ ] Verify summary shows correct totals
- [ ] Test error handling: add invalid query ID to plan

## File Changes Summary (Phase 2)

| File                             | Action |
|----------------------------------|--------|
| `internal/exec/hash.go`          | Create |
| `internal/exec/hash_test.go`     | Create |
| `internal/exec/writer.go`        | Create |
| `internal/exec/writer_test.go`   | Create |
| `internal/exec/executor.go`      | Modify |
| `internal/exec/executor_test.go` | Modify |
| `internal/command/exec.go`       | Modify |

## Notes (Phase 2)

- Model hash uses first 8 chars of SHA-256 for short, collision-resistant names
- Response files use `{query_id}_response.md` naming pattern
- Execution continues on errors, collecting all errors in summary
- DryRun shows full execution matrix with output paths
- Console output shows progress as `✓ query → path` or `✗ error`
- Token totals are aggregated across all executions
