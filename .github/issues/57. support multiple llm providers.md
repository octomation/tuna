---
issue: 57
status: open
type: task
labels:
  - "effort: medium"
  - "impact: high"
  - "scope: code"
  - "type: feature"
assignees:
  - kamilsk
milestone: null
projects: []
relationships:
  parent: null
  blocked_by: []
  blocks: []
---

# Support Multiple LLM Providers

## Summary

Implement support for multiple LLM providers (OpenRouter, Anthropic, OpenAI, etc.) with a unified client that routes requests to the appropriate provider based on model name.

## Motivation

Currently, tuna supports only a single LLM provider configured via environment variables (`LLM_API_TOKEN`, `LLM_BASE_URL`). This limits the ability to:

- Compare responses from models hosted on different providers in a single execution
- Use specialized providers for specific models (e.g., Anthropic API for Claude, OpenAI API for GPT)
- Leverage cost-effective aggregators like OpenRouter alongside direct provider access

## Current State

```go
// internal/llm/client.go
type Config struct {
    APIToken string
    BaseURL  string
}

func ConfigFromEnv() (*Config, error) {
    token := os.Getenv("LLM_API_TOKEN")
    baseURL := os.Getenv("LLM_BASE_URL")
    // ...
}
```

**Limitations:**
- Single provider per execution
- Environment variables don't scale for multiple providers
- No model-to-provider mapping

## Proposed Solution

### 1. Configuration File

Replace environment variables with a TOML configuration file (`~/.config/tuna.toml` or `.tuna.toml` in project root):

```toml
# Default provider used when model is not found in any provider's model list
default_provider = "openrouter"

# Model aliases for convenience (short name -> full model name)
[aliases]
sonnet = "claude-sonnet-4-20250514"
haiku = "claude-haiku-3-5-20241022"
gpt4 = "gpt-4o"
gpt4-mini = "gpt-4o-mini"
llama = "meta-llama/llama-3.3-70b-instruct"

[[providers]]
name = "openrouter"
base_url = "https://openrouter.ai/api/v1"
api_token_env = "OPENROUTER_API_KEY"  # Reference to env variable
rate_limit = "10rpm"  # 10 requests per minute
models = [
    "anthropic/claude-sonnet-4",
    "openai/gpt-4o",
    "google/gemini-2.0-flash",
    "meta-llama/llama-3.3-70b-instruct",
]

[[providers]]
name = "anthropic"
base_url = "https://api.anthropic.com/v1"
api_token_env = "ANTHROPIC_API_KEY"
rate_limit = "60rpm"  # Anthropic tier-based limit
models = [
    "claude-sonnet-4-20250514",
    "claude-haiku-3-5-20241022",
]

[[providers]]
name = "openai"
base_url = "https://api.openai.com/v1"
api_token_env = "OPENAI_API_KEY"
rate_limit = "500rpm"  # OpenAI tier-based limit
models = [
    "gpt-4o",
    "gpt-4o-mini",
    "o1",
]
```

### 2. Configuration Structure

```go
// internal/config/config.go

type Config struct {
    DefaultProvider string            `toml:"default_provider"`
    Aliases         map[string]string `toml:"aliases"`  // short name -> full model name
    Providers       []Provider        `toml:"providers"`
}

type Provider struct {
    Name        string   `toml:"name"`
    BaseURL     string   `toml:"base_url"`
    APITokenEnv string   `toml:"api_token_env"`
    RateLimit   string   `toml:"rate_limit"` // e.g., "10rpm", "5rps" (empty = unlimited)
    Models      []string `toml:"models"`
}

// RateLimit represents a parsed rate limit value.
type RateLimit struct {
    Value int           // Number of requests
    Unit  time.Duration // Per unit of time (time.Second, time.Minute, time.Hour)
}

// ParseRateLimit parses rate limit string like "10rpm", "5rps", "100rph".
// Supported units: rps (per second), rpm (per minute), rph (per hour).
func ParseRateLimit(s string) (*RateLimit, error) {
    // Parse format: "<number><unit>" (e.g., "10rpm")
    // Returns nil if empty string (unlimited)
}
```

### 3. Router Client

Create a unified client that routes requests to the appropriate provider:

```go
// internal/llm/router.go

type Router struct {
    providers       map[string]*Client        // name -> client
    rateLimiters    map[string]*rate.Limiter  // name -> rate limiter
    aliases         map[string]string         // alias -> full model name
    modelMapping    map[string]string         // model -> provider name
    defaultProvider string
}

func NewRouter(cfg *config.Config) (*Router, error) {
    // Build provider clients, rate limiters, and model mapping
    // For each provider with rate_limit set:
    //   limit := config.ParseRateLimit(provider.RateLimit)
    //   rate.NewLimiter(rate.Every(limit.Unit/limit.Value), 1)
}

func (r *Router) Chat(ctx context.Context, req ChatRequest) (*ChatResponse, error) {
    provider := r.resolveProvider(req.Model)

    // Wait for rate limiter if configured
    if limiter, ok := r.rateLimiters[provider]; ok {
        if err := limiter.Wait(ctx); err != nil {
            return nil, fmt.Errorf("rate limit wait cancelled: %w", err)
        }
    }

    client := r.providers[provider]
    return client.Chat(ctx, req)
}

func (r *Router) resolveProvider(model string) string {
    // First, resolve alias to full model name
    resolvedModel := r.resolveAlias(model)

    if provider, ok := r.modelMapping[resolvedModel]; ok {
        return provider
    }
    return r.defaultProvider
}

func (r *Router) resolveAlias(model string) string {
    if fullName, ok := r.aliases[model]; ok {
        return fullName
    }
    return model  // Return as-is if not an alias
}
```

### 4. Configuration Loading Priority

1. Project-level: `.tuna.toml` in current directory or parent directories
2. User-level: `~/.config/tuna.toml`
3. Fallback: Environment variables (backward compatibility)

### 5. CLI Changes

Add `config` subcommand for configuration management:

```bash
# Show current configuration
tuna config show

# Validate configuration
tuna config validate

# Show which provider will be used for a model
tuna config resolve <model>
```

## Implementation Plan

### Phase 1: Configuration Infrastructure

| File                             | Action | Description                              |
|----------------------------------|--------|------------------------------------------|
| `internal/config/config.go`      | Create | Configuration structures                 |
| `internal/config/loader.go`      | Create | TOML loading with priority resolution    |
| `internal/config/loader_test.go` | Create | Configuration loading tests              |

### Phase 2: Router Client

| File                           | Action | Description                                |
|--------------------------------|--------|--------------------------------------------|
| `internal/llm/router.go`       | Create | Multi-provider router implementation       |
| `internal/llm/router_test.go`  | Create | Router tests with mocked providers         |
| `internal/llm/client.go`       | Modify | Extract interface, keep as single provider |

### Phase 3: Integration

| File                           | Action | Description                                |
|--------------------------------|--------|--------------------------------------------|
| `internal/command/exec.go`     | Modify | Use Router instead of single Client        |
| `internal/exec/executor.go`    | Modify | Accept Router interface                    |
| `internal/command/config.go`   | Create | Config subcommand implementation           |

### Phase 4: Documentation & Migration

| File                           | Action | Description                                |
|--------------------------------|--------|--------------------------------------------|
| `CLAUDE.md`                    | Modify | Update configuration docs                  |
| `README.md`                    | Modify | Add multi-provider setup guide             |

## Interface Design

```go
// internal/llm/interface.go

// ChatClient defines the interface for LLM chat operations.
type ChatClient interface {
    Chat(ctx context.Context, req ChatRequest) (*ChatResponse, error)
}

// Ensure both Client and Router implement ChatClient
var _ ChatClient = (*Client)(nil)
var _ ChatClient = (*Router)(nil)
```

## Error Handling

1. **Missing provider for model**: Use default provider with warning log
2. **Missing API token**: Clear error message with env variable name
3. **Invalid configuration**: Validation errors on load with line numbers
4. **Provider unavailable**: Retry with exponential backoff (existing behavior)
5. **Rate limit exceeded**: Block and wait until the next request slot is available (using `golang.org/x/time/rate`)

## Backward Compatibility

- If no config file exists and `LLM_API_TOKEN`/`LLM_BASE_URL` are set, create an implicit "default" provider
- Deprecation warning when using environment variables only
- Migration guide in documentation

## Example Usage

```bash
# Plan with aliases - much shorter than full model names
tuna plan MyAssistant --models "sonnet,gpt4,llama"

# Or mix aliases with full names
tuna plan MyAssistant \
    --models "sonnet,gpt-4o-mini,anthropic/claude-sonnet-4"

# Execute - router resolves aliases and routes to correct providers
tuna exec abc123

# Check alias and provider resolution
tuna config resolve sonnet
# Output: claude-sonnet-4-20250514 -> anthropic

tuna config resolve gpt4
# Output: gpt-4o -> openai

tuna config resolve unknown-model
# Output: unknown-model -> openrouter (default provider)
```

## Testing Strategy

1. **Unit tests**: Config loading, router logic, provider resolution
2. **Integration tests**: Mock HTTP servers for each provider type
3. **E2E tests**: Real API calls with test credentials (optional, CI secrets)

## Acceptance Criteria

- [ ] Configuration file is loaded from `.tuna.toml` or `~/.config/tuna.toml`
- [ ] Multiple providers can be defined with their own credentials
- [ ] Models are automatically routed to the correct provider
- [ ] Default provider is used for unknown models
- [ ] Backward compatibility with environment variables
- [ ] `tuna config show/validate/resolve` commands work correctly
- [ ] Rate limiting works per provider (configurable via `rate_limit`)
- [ ] Model aliases resolve to full model names before provider lookup
- [ ] Existing tests pass, new tests cover multi-provider scenarios

---

## Implementation Plan

### Phase 1: Configuration Infrastructure

**Goal**: Create TOML configuration loading system with priority resolution.

#### 1.1. Configuration Structures

**File**: `internal/config/config.go`

```go
// Config represents the root tuna configuration.
type Config struct {
    DefaultProvider string            `toml:"default_provider"`
    Aliases         map[string]string `toml:"aliases"`
    Providers       []Provider        `toml:"providers"`
}

// Provider describes a single LLM provider configuration.
type Provider struct {
    Name        string   `toml:"name"`
    BaseURL     string   `toml:"base_url"`
    APITokenEnv string   `toml:"api_token_env"`
    RateLimit   string   `toml:"rate_limit"`
    Models      []string `toml:"models"`
}

// RateLimit represents a parsed rate limit value.
type RateLimit struct {
    Value int
    Unit  time.Duration
}

// ParseRateLimit parses strings like "10rpm", "5rps", "100rph".
func ParseRateLimit(s string) (*RateLimit, error)
```

**Tasks**:
1. Create file `internal/config/config.go`
2. Implement structures `Config`, `Provider`, `RateLimit`
3. Implement `ParseRateLimit()` with rps/rpm/rph support
4. Add `Validate()` method for configuration validation

#### 1.2. Configuration Loader

**File**: `internal/config/loader.go`

```go
// Load loads configuration with priority:
// 1. .tuna.toml in current/parent directories
// 2. ~/.config/tuna.toml
// 3. Fallback to env variables (backward compatibility)
func Load() (*Config, error)

// LoadFromFile loads configuration from a specific file.
func LoadFromFile(path string) (*Config, error)

// findConfigFile searches for .tuna.toml up the directory tree.
func findConfigFile() (string, error)
```

**Tasks**:
1. Create file `internal/config/loader.go`
2. Implement `.tuna.toml` search up the directory tree
3. Implement fallback to `~/.config/tuna.toml`
4. Implement fallback to env variables for backward compatibility
5. Add deprecation warning when using env variables only

#### 1.3. Configuration Tests

**File**: `internal/config/config_test.go`

**Tasks**:
1. Tests for `ParseRateLimit()` - valid values (10rpm, 5rps, 100rph)
2. Tests for `ParseRateLimit()` - invalid values
3. Tests for `Config.Validate()` - valid configuration
4. Tests for `Config.Validate()` - missing default_provider
5. Tests for `Config.Validate()` - duplicate provider names

**File**: `internal/config/loader_test.go`

**Tasks**:
1. Test loading from `.tuna.toml`
2. Test loading from `~/.config/tuna.toml`
3. Test priority (project file takes precedence over global)
4. Test fallback to env variables
5. Test file search up the directory tree

---

### Phase 2: Router Client

**Goal**: Create a router that directs requests to the appropriate provider.

#### 2.1. ChatClient Interface

**File**: `internal/llm/interface.go`

```go
// ChatClient defines the interface for LLM operations.
type ChatClient interface {
    Chat(ctx context.Context, req ChatRequest) (*ChatResponse, error)
}

// Compile-time interface implementation checks
var _ ChatClient = (*Client)(nil)
var _ ChatClient = (*Router)(nil)
```

**Tasks**:
1. Create file `internal/llm/interface.go`
2. Define `ChatClient` interface
3. Add compile-time implementation checks

#### 2.2. Router

**File**: `internal/llm/router.go`

```go
// Router routes requests to appropriate providers.
type Router struct {
    providers       map[string]*Client
    rateLimiters    map[string]*rate.Limiter
    aliases         map[string]string
    modelMapping    map[string]string
    defaultProvider string
}

// NewRouter creates a router from configuration.
func NewRouter(cfg *config.Config) (*Router, error)

// Chat sends a request to the appropriate provider.
func (r *Router) Chat(ctx context.Context, req ChatRequest) (*ChatResponse, error)

// resolveProvider determines the provider for a model.
func (r *Router) resolveProvider(model string) string

// resolveAlias resolves an alias to the full model name.
func (r *Router) resolveAlias(model string) string

// ResolveModel returns full model name and provider (for CLI).
func (r *Router) ResolveModel(model string) (fullName, provider string)
```

**Tasks**:
1. Create file `internal/llm/router.go`
2. Implement `NewRouter()` - create clients for each provider
3. Implement rate limiter creation for providers with limits
4. Build `modelMapping` from provider model lists
5. Implement `Chat()` with rate limiter wait
6. Implement `resolveAlias()` and `resolveProvider()`
7. Implement `ResolveModel()` for `tuna config resolve` command

#### 2.3. Router Tests

**File**: `internal/llm/router_test.go`

**Tasks**:
1. Test routing to correct provider
2. Test fallback to default provider for unknown models
3. Test alias resolution
4. Test rate limiting (verify blocking on limit exceeded)
5. Test router creation with invalid configuration
6. Test with mock HTTP server for actual requests

---

### Phase 3: Integration

**Goal**: Integrate Router into existing code.

#### 3.1. Executor Modification

**File**: `internal/exec/executor.go`

**Changes**:
```go
// Before:
type Executor struct {
    llmClient *llm.Client
}

// After:
type Executor struct {
    llmClient llm.ChatClient  // interface instead of concrete type
}
```

**Tasks**:
1. Change `llmClient` field type to `llm.ChatClient`
2. Update `New()` constructor to accept interface
3. Verify remaining code requires no changes

#### 3.2. Exec Command Modification

**File**: `internal/command/exec.go`

**Changes**:
```go
// Before:
llmCfg, err := llm.ConfigFromEnv()
llmClient := llm.NewClient(llmCfg)

// After:
cfg, err := config.Load()
router, err := llm.NewRouter(cfg)
```

**Tasks**:
1. Replace `llm.ConfigFromEnv()` with `config.Load()`
2. Replace `llm.NewClient()` with `llm.NewRouter()`
3. Update command description (remove env variable references)
4. Add informative error messages for configuration errors

#### 3.3. Config Command

**File**: `internal/command/config.go`

```go
// Config returns a cobra.Command for configuration management.
func Config() *cobra.Command

// configShow displays current configuration.
func configShow() *cobra.Command

// configValidate validates configuration.
func configValidate() *cobra.Command

// configResolve shows provider for a model.
func configResolve() *cobra.Command
```

**Tasks**:
1. Create file `internal/command/config.go`
2. Implement `tuna config show` - display current configuration
3. Implement `tuna config validate` - validation with detailed errors
4. Implement `tuna config resolve <model>` - show alias → model → provider
5. Register command in `root.go`

#### 3.4. Root.go Modification

**File**: `internal/command/root.go`

**Tasks**:
1. Add `Config()` to subcommands list

---

### Phase 4: Documentation and Migration

**Goal**: Update documentation and ensure smooth migration.

#### 4.1. Update CLAUDE.md

**File**: `CLAUDE.md`

**Tasks**:
1. Replace Configuration section with TOML configuration description
2. Add configuration examples for different scenarios
3. Describe configuration loading priority
4. Add `tuna config` command description

#### 4.2. Configuration Example

**File**: `.tuna.toml.example`

**Tasks**:
1. Create file with full configuration example
2. Add comments to each section

---

### Task Execution Order

| #  | Task                                        | Depends On | Files                                   |
|----|---------------------------------------------|------------|-----------------------------------------|
| 1  | Create configuration structures             | -          | `internal/config/config.go`             |
| 2  | Implement `ParseRateLimit()`                | 1          | `internal/config/config.go`             |
| 3  | Implement `Config.Validate()`               | 1          | `internal/config/config.go`             |
| 4  | Write tests for config.go                   | 1-3        | `internal/config/config_test.go`        |
| 5  | Implement configuration loader              | 1          | `internal/config/loader.go`             |
| 6  | Write tests for loader.go                   | 5          | `internal/config/loader_test.go`        |
| 7  | Create ChatClient interface                 | -          | `internal/llm/interface.go`             |
| 8  | Implement Router                            | 1, 7       | `internal/llm/router.go`                |
| 9  | Write tests for Router                      | 8          | `internal/llm/router_test.go`           |
| 10 | Modify Executor (interface)                 | 7          | `internal/exec/executor.go`             |
| 11 | Modify exec command                         | 5, 8, 10   | `internal/command/exec.go`              |
| 12 | Create config command                       | 5, 8       | `internal/command/config.go`            |
| 13 | Register config command                     | 12         | `internal/command/root.go`              |
| 14 | Update CLAUDE.md                            | 11-13      | `CLAUDE.md`                             |
| 15 | Create configuration example                | 14         | `.tuna.toml.example`                    |

---

### Checkpoints

Verify after each phase:

**Phase 1**:
- [ ] `go test ./internal/config/...` passes
- [ ] Configuration loads from `.tuna.toml`
- [ ] Fallback to env variables works

**Phase 2**:
- [ ] `go test ./internal/llm/...` passes
- [ ] Router correctly routes requests
- [ ] Rate limiting blocks on limit exceeded

**Phase 3**:
- [ ] `go test ./...` passes (all existing tests)
- [ ] `tuna exec` works with new configuration
- [ ] `tuna config show/validate/resolve` work

**Phase 4**:
- [ ] Documentation is up to date
- [ ] Configuration example is correct
